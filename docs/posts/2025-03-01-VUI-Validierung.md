---
title: "Verstehen Sprachassistenten uns wirklich? Neue UX-Studie zeigt, wie man dies wissenschaftlich misst."
date: 2025-03-01
categories:
  - Publikation
  - UX-Forschung
  - Sprachassistenten
  - User Experience (UX)
tags:
  - UEQ+
  - Sprachsteuerung
  - Voice User Interface (VUI)
  - UX Messung
  - Validierung
  - KI-Interaktion
---

![Sprachassistenten UX-Studie](assets/2025-03-01-article-VUI.png){align=right width="30%"}


*Klein, Andreas M.; Kollmorgen, Jessica; Hinderks, Andreas; Schrepp, Martin; Rauschenberger, Maria; Escalona, Maria-Jose (2025): **Validation of the UEQ+ Scales for Voice Quality**. In: Computer Standards & Interfaces, Vol. 93.*, Doi: 10.1016/j.csi.2024.103971 **||** [Download](https://doi.org/10.1016/j.csi.2024.103971)

### Zusammenfassung  

Sprachassistenten wie Alexa, Siri und Google Assistant sind aus dem Alltag vieler Menschen nicht mehr wegzudenken. Doch wie gut ist ihre User Experience (UX) wirklich? Der Artikel untersucht, wie sich die UX-QualitÃ¤t von Voice User Interfaces (VUIs) messen lÃ¤sst. DafÃ¼r wurden drei neue Skalen fÃ¼r das **UEQ+ Framework** entwickelt: **â€Response Behaviorâ€œ**, **â€Response Qualityâ€œ** und **â€Comprehensibilityâ€œ**. Diese Skalen erfassen, wie natÃ¼rlich sich die Sprachassistenten verhalten, wie prÃ¤zise ihre Antworten sind und wie gut sie gesprochene Befehle verstehen.  

In einer groÃŸ angelegten Studie mit **623 Teilnehmer*innen** aus den USA und GroÃŸbritannien konnte das Forschungsteam zeigen, dass diese Skalen zuverlÃ¤ssig und valide sind. Besonders spannend: Die Ergebnisse zeigen nicht nur StÃ¤rken der Systeme, sondern auch klare VerbesserungsmÃ¶glichkeiten â€“ etwa bei der DialogverstÃ¤ndlichkeit oder der Datensicherheit. Damit liefert die Studie eine wertvolle Grundlage fÃ¼r die Weiterentwicklung smarter Sprachassistenten.  

<!-- more -->

### Wissenschaftliche Fakten  

- **Studienziel:** Validierung der neuen UEQ+ Skalen fÃ¼r Voice User Interfaces (VUIs).  
- **Untersuchte Systeme:** Alexa (Amazon), Siri (Apple), Google Assistant (Google).  
- **Stichprobe**  
    - DurchgefÃ¼hrt Februar-MÃ¤rz 2023 
    - Gesamtzahl der Teilnehmer:innen: **623** (nach Datenbereinigung).  
    - Herkunft: **USA und GroÃŸbritannien**.  
    - Geschlechterverteilung: **306 weiblich, 304 mÃ¤nnlich, 9 divers, 4 ohne Angabe**.  
    - Durchschnittsalter: **35,6 Jahre (SD = 12,2 Jahre)**.  
- **Methode:** Bewertung basierte auf 5 UX-Aspekten aus dem **UEQ+ Framework**:  
    - **Stimulation** (SpaÃŸ an der Nutzung)  
    - **Intuitive Use** (Intuitive Bedienbarkeit)  
    - **Response Behavior** (NatÃ¼rlichkeit des Sprachverhaltens)  
    - **Response Quality** (QualitÃ¤t der Antworten)  
    - **Comprehensibility** (VerstÃ¤ndlichkeit der Sprachbefehle)  
- **Wichtige Ergebnisse**  
    - **Hohe interne Konsistenz:** Cronbachs Alpha-Werte fÃ¼r die drei Voice-Skalen zwischen **0,85 und 0,92**.  
    - **Faktorielle ValiditÃ¤t bestÃ¤tigt:** Items der drei Voice-Skalen laden eindeutig auf je einem Faktor.  
    - **Durchschnittliche UX-Bewertung der Sprachassistenten (Skala von -3 bis +3):**  
        - HÃ¶chste Werte fÃ¼r **Intuitive Use** (Alexa: 1,59; Siri: 1,47; Google Assistant: 1,56).  
        - Niedrigste Werte fÃ¼r **Response Behavior** (Alexa: 0,59; Siri: 0,47; Google Assistant: 0,67).  
- **HÃ¤ufigste VerbesserungsvorschlÃ¤ge**  
    - **Bessere DialogverstÃ¤ndlichkeit** (107 Nennungen fÃ¼r Alexa, 126 fÃ¼r Siri, 109 fÃ¼r Google Assistant).  
    - **Genauere Erkennung der Nutzerintention**  
    - **HÃ¶here Datensicherheit**
    - **NatÃ¼rlichere SprachfÃ¼hrung (menschlichere Konversation)**  

---

Abbildungen aus dem wissenschaftlichen Artikel  

- **Methodische Schritte zur Validierung der UEQ+ Skalen:**  

  ![Abbildung 1: Methodologie-Schritte](assets/2025-03-01-VUI-Abb1.jpg){width="80%"}  


- **UX-Bewertung der Sprachassistenten anhand der UEQ+ Skalen:**  

  ![Abbildung 5: UX-Bewertung](assets/2025-03-01-VUI-Abb5.jpg){width="50%"}  

- **VerbesserungsvorschlÃ¤ge der Teilnehmer*innen:**  

  ![Abbildung 7: VerbesserungsvorschlÃ¤ge](assets/2025-03-01-VUI-Abb7.jpg){width="90%"} 



### â€Siri, warum verstehst du mich nicht?â€œ  

Eine einfache Wissensfrage an Siri: â€Wie viele Einwohner hat Hamburg?â€œ wurde wie folgt korrekt beantwortet: â€2022 hatte Hamburg 1.892.122 Einwohner.â€œ Die erste Folgefrage: â€Und in Sevilla?â€œ wurde ebenfalls korrekt beantwortet: â€2023 hatte Sevilla 684.025 Einwohner.â€œ Auf die zweite Folgefrage â€Und Madrid?â€œ antwortete Siri nicht korrekt: â€Das habe ich im Internet gefunden.â€œ Folgen des Dialogkontextes, d.h. erfassen von natÃ¼rlicher Sprache und typischen Dialogen, z.B. mit Folgefragen, ist fÃ¼r Siri schwierig. 

Diese und weitere Interaktionsbarrieren (z.B. verwenden von Eigennamen oder sprechen mit Akzent) veranlassten die Forschenden zur neuen Studie und der Messung der User Experience von Sprachassistenten. Sie wollten wissen: Wie gut funktionieren Alexa, Siri und Google Assistant wirklich? Und wie lassen sich ihre StÃ¤rken und SchwÃ¤chen erfassen?  

DafÃ¼r entwickelten sie drei spezielle Skalen, um die SprachqualitÃ¤t dieser Systeme zu bewerten: **â€Response Behaviorâ€œ** (= **Antwortverhalten**; wie natÃ¼rlich sich die Assistenten verhalten), **â€Response Qualityâ€œ** (= **AntwortqualitÃ¤t**; wie prÃ¤zise ihre Antworten sind) und **â€Comprehensibilityâ€œ** (= **VerstÃ¤ndlichkeit**; wie gut sie gesprochene Befehle verstehen). **623 Nutzer*innen** aus den USA und GroÃŸbritannien nahmen an der Untersuchung teil â€“ und die Ergebnisse waren aufschlussreich.  

WÃ¤hrend die Assistenten in puncto **â€Intuitive Useâ€œ** gut abschnitten (die meisten Nutzer:innen fanden sie leicht bedienbar), gab es klare Defizite bei der **VerstÃ¤ndlichkeit der Dialoge**. Besonders Siri wurde fÃ¼r ihre **schwache Kontext-Erkennung** kritisiert, wÃ¤hrend Google Assistant bei der AntwortqualitÃ¤t leicht vorne lag. Ein hÃ¤ufig genannter Wunsch: **NatÃ¼rlichere GesprÃ¤che und weniger MissverstÃ¤ndnisse**.  

### ğŸ¯ Fazit  

Die Studie zeigt, dass die neuen UEQ+ Skalen eine zuverlÃ¤ssige Methode zur Bewertung der SprachqualitÃ¤t von Assistenten wie Alexa, Siri und Google Assistant darstellen. Gleichzeitig offenbaren die Ergebnisse klare Verbesserungspotenziale â€“ besonders im Bereich der SprachverstÃ¤ndlichkeit und der natÃ¼rlichen DialogfÃ¼hrung. Diese Erkenntnisse bieten wertvolle AnsÃ¤tze fÃ¼r die Weiterentwicklung smarter Sprachassistenten. 

Damit sind nun die folgenden drei UEQ+ Skalen fÃ¼r Sprachanwendungen validiert: **Response Behavior** (= **Antwortverhalten**), **Response Quality** (= **AntwortqualitÃ¤t**) und **Comprehensibility** (= **VerstÃ¤ndlichkeit**)

Die Skalen werden bereits in der Forschung und Entwicklung, z.B. zur Evaluation von Pro-aktiven Fahrzeugassistenzsystemen mit LLMs, verwendet.

---


???+ tip "Autor:innen aus dem "Forschen-im-Norden.de"-Team"

    --8<-- "andreas_klein.md"

    ---  

    --8<-- "jessica_kollmorgen.md"

    --- 

    --8<-- "andreas_hinderks.md"

    --- 

    --8<-- "maria_rauschenberger.md"